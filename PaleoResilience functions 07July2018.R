#######################################
# Paleoecological Resilience Indicator functions
# Stegner et al. 
# updated 08 July 2018
#######################################

# PACKAGES  #####################################################################
library(breakpoint)
library(stats)
library(moments)

####FUNCTIONS###################################################################

# trimtoRS2________________________________________________________________
# time a time series to the portion prior to regime shift, using breakpoint analysis
# returns a two column vector, with data from prior to the regime shift

trimtoRS2<-function(TS,cutoff,cutoff2,trim.type,start){
	# INPUTS:
	# TS = 2 col matrix, with [,1] as time and [,2] as ecological variable
	# cutoff = amount of time to retain prior to regime shift 
	# cutoff2 = amount of time to retain after regime shift
	# trim.type = should the time of regime shift be determined analytically ("to.RS"), arbitarily ("to.set.bounds"), or should no trimming take place ("none")?
	# start = if trim.type = "to.set.bounds", start is the first time step to include
	
	# RETURN:
	# trimTS = trimmed time series 2 col matrix, with [,1] as time and [,2] as ecological variable
	# tail.length = amount of time trimmed from the beginning of the time series, used for lining up trimmed and untrimmed versions
	
	if (trim.type=="to.RS"){
		bp.out<-CE.Normal.Mean(as.data.frame(TS[,2]),Nmax=1)
		timeCT<-TS[bp.out$BP.Loc,1]
		trimTSt<-TS[which(TS[,1]<timeCT),]
		trimTS<-TS[c((timeCT-cutoff):(timeCT+cutoff2)),]
		tail.length<-nrow(trimTSt)-cutoff
	} else if (trim.type=="to.set.bounds"){
		trimTS<-TS[c(start:(start+cutoff+cutoff2)),]
		tail.length<-start
	} else if (trim.type=="none"){
		trimTS<-TS
		tail.length<-nrow(TS)
	} else {
		print("trim.type must be 'to.RS','set.bounds', or 'none'")
	}
	out<-list(trimTS=trimTS,tail.length=tail.length)
	return(out)
}

# brokenstick.timeavgTS_______________________________________________________
# This function applies time averaging: two constant rates of time averaging, with a single breakpoint (expressed as a time step) at which time averaging changes from one rate to the other. Also applies a small amount of noise around the # of years per cm, and calculates of centimeters in the core

brokenstick.timeavgTS<-function(oTS,TAbottom,TAtop,breakpoint,sd.pct){
	# INPUTS:
	# oTS = generally, a 2 col matrix, with [,1] as time and [,2] as ecological variable. An untransformed single run of the GW model
	# TAbottom = time averaging in number of years per cm at the bottom (older) of the core
	# TAtop = time averaging in number of years per cm at the top (younger) of the core
	# breakpoint = time step (as a irow number index) of transition from TAbottom to TAtop 
	# sd.pct = determines standard deviation of noise around time averaging, as a % of the mean 
	
	# RETURN:
	# adTS = a two colum matrix, with time as [,1] (oldest age of the time aberaged bin) and time-averaging ecological variable as [,2]
	# TAbins = vector of time averaging durations generated by rnorm sampling of TAvect
	# TAvect = vector of mean time averaging used to generate TAbins
	
	TS<-oTS[,2]
	
	# divide the time series in half, according to the breakpoint 
	TSold<-TS[1:breakpoint]
	TSyoung<-TS[(breakpoint+1):length(TS)]
	
	# create a vector of mean time averaging values that is the same length as the time series
	TAvec<-c(rep(TAbottom,length(TSold)),rep(TAtop,length(TSyoung)))
	
	# add noise to the time averaging vector
	rTA<-c()
	for (i in 1:length(TAvec)){
		rTA[i]<-round(rnorm(1,TAvec[i],sd.pct*TAvec[i]))
	}
	rTA[which(rTA==0)]<-1

	
	TAbins<-rep(0,length(TS))
	TAbins[1]<-rTA[1]
	for (i in 2:length(TS)) {
		TAbins[i]<-rTA[sum(TAbins)+1]
		if (sum(TAbins,na.rm=T)>length(TS)) break
	}
	
	TAbins<-TAbins[which(TAbins>0)]
	cTAbins<-c(1,cumsum(TAbins))
	cTAbins<-cTAbins[which(cTAbins<length(TS))]

	TAts<-c()
	for (j in 1:(length(cTAbins)-1)){
		TAts[j]<-mean(TS[cTAbins[j]:cTAbins[(j+1)]])
	}
	
	adTS<-cbind((cTAbins[1:length(TAts)]),TAts)
	out<-list(adTS=adTS,TAbins=TAbins,TAvect=rev(TAvec))
	return(out)
}

# timeavgTS___________________________________________________________________________
# applies time averaging as a slow linear change from one rate to another, adds a small amount of noise around the # of years per cm, and calculates # of centimeters in the core

timeavgTS<-function(oTS,TAbottom,TAtop,sd.pct){
	# INPUTS:
	# oTS = generally, a 2 col matrix, with [,1] as time and [,2] as ecological variable. An untransformed single run of the GW model
	# TAbottom = time averaging in number of years per cm at the bottom (older) of the core
	# TAtop = time averaging in number of years per cm at the top (younger) of the core
	# sd.pct = determines standard deviation of noise around time averaging, as a % of the mean 
	
	# RETURN:
	# adTS = a two colum matrix, with time as [,1] (oldest age of the time aberaged bin) and time-averaging ecological variable as [,2]
	# TAbins = vector of time averaging durations generated by rnorm sampling of TAvect
	# TAvect = vector of mean time averaging used to generate TAbins
	
	TS<-oTS[,2]

	m<-(TAbottom-TAtop)/(length(TS)-1)
	b<-TAtop-m

	TA<-c()
	rTA<-c()
	for (i in 1:length(TS)){
		TA[i]<-m*i+b
		rTA[i]<-round(rnorm(1,TA[i],sd.pct*TA[i]))
	}
	rTA[which(rTA<=0)]<-1

	TAbins<-c()
	TAbins[1]<-rTA[1]
	for (i in 2:length(TS)) {
		TAbins[i]<-rTA[sum(TAbins)+1]
		if (sum(TAbins,na.rm=T)>length(TS)) break
	}
	
	TAbins<-TAbins[which(TAbins>0)]
	cTAbins<-c(1,cumsum(TAbins))
	cTAbins<-cTAbins[which(cTAbins<length(TS))]

	TAts<-c()
	for (j in 1:(length(cTAbins)-1)){
		TAts[j]<-mean(TS[cTAbins[j]:cTAbins[(j+1)]])
	}
	
	adTS<-cbind((cTAbins[1:length(TAts)]),TAts)
	out<-list(adTS=adTS,TAbins=TAbins,TAvect=TA)
	return(out)
}

# Carpenter_timeavgTS____________________________________________________________________
# compresses a time series using exponential sedimentation, according to Taranu et al. (in review) method

Carpenter_timeavgTS<-function(oTS,a0,a1,tail){
	# INPUTS:
	# oTS = generally, a 2 col matrix, with [,1] as time and [,2] as ecological variable. An untransformed single run of the GW model
	# a0 = compression intercepts parameter
	# a1 = compression slope parameter
	# tail = tail.length from trimtoRS2; can be set to 0 if time series has not been trimmed
	
	# RETURN:
	# adTS = a two colum matrix, with time as [,1] (oldest age of the time aberaged bin) and time-averaging ecological variable as [,2]
	# TAbins = vector of time averaging durations generated by rnorm sampling of TAvect
	# TAvect = vector of mean time averaging used to generate TAbins
	
	TS<-oTS[,2]
	
	# reverse the time series to start at top of core
	Wmix.rev = rev(TS) 
	gens<-length(Wmix.rev)
		
	# Compute core length in cm
	L.core = (1/a1)*( log(a1*gens + a0) - log(a0))
	
	# Discard non-integer fraction and end of core
	L.core = floor(L.core)
	
	# Average tracer concentrations for each 1 cm slice of core
	core = rep(0,L.core)
	t0 = 0 # start at the top of the core
	t1 = 0
	# save matrix of t0 and t1 values to check results
	t01mat = matrix(0,nr=L.core,nc=2)
	
	# Do the first slice separately to accommodate starting at exactly 0
	# Compute new t1 from forumula in 'Pseudocode_Mixing+Compression_2016-08-25.doc'
	t1 = (1/a1)*( (a1*t0 + a0)*exp(a1) - a0)
	# Save t0 and t1
	t01mat[1,]=c(t0,t1)
	# average the tracer concentrations between t0 and t1
	y.slice = ceiling(t1)-floor(t0)
	wts = rep(1,y.slice)
	x = Wmix.rev[1:ceiling(t1)]
	wts[1] = ceiling(t0)-t0 # fraction of the first year in the slice
	wts[y.slice] = t1 - floor(t1) # fraction of the final year in the slice
	core[1] = x%*%wts/sum(wts)
	
	# Do the remaining slices
	for(i in 2:L.core) {  # loop over core slices
		t0 = t1 # previous t1 is now t0
		# Compute new t1 from forumula in 'Pseudocode_Mixing+Compression_2016-08-25.doc'
		t1 = (1/a1)*( (a1*t0 + a0)*exp(a1) - a0)
		# Save t0 and t1
		t01mat[i,]=c(t0,t1)
		# average the tracer concentrations between t0 and t1
		y.slice = ceiling(t1)-floor(t0)+1
		wts = rep(1,y.slice)
		x = Wmix.rev[floor(t0):ceiling(t1)]
		wts[1] = ceiling(t0)-t0 # fraction of the first year in the slice
		wts[y.slice] = t1 - floor(t1) # fraction of the final year in the slice
		core[i] = x%*%wts/sum(wts)
	}
		
		# Compute times at the center of each core slice
		t01mat[1,1]=1.e-3 # replace the zero to prevent problem with log
		T.core = exp( 0.5*(log(t01mat[,1])+log(t01mat[,2])) )
		# reverse the sequence of times to correspond with the simulated series
		T.core = (gens - floor(T.core))+tail
		
		adTS<-cbind(rev(T.core),rev(core))
		TAbins<-cbind(rev(t01mat[,1]),rev(t01mat[,2]))
		TAvect=TAbins[,2]-TAbins[,1]
		out<-list(adTS=adTS,TAbins=TAbins,TAvect=rev(TAvect))
		return(out)
}

# sampleTS___________________________________________________________________________
# this function subsamples a time series at regular intervals

sampleTS<-function(TS2,sample.by,samp.freq1,nsamp,timeCT){
	# INPUTS:
	# TS2 = a matrix with 2 cols, [,1] is ages, [,2] is ecological value, e.g. tree cover
	# sample.by = should sampling occur at predefined depth increments ("depth") across entire time sereis, or should a set number of samples be distrbuted at regular increments ("distribute.samples") prior to timeCT?
	# sample.freq1 = an integer, increment at which to sample TS2 if sample.by="depth"
	# nsamp = an integer, number of samples of TS2 to take is sample.by="distribute.samples"
	# timeCT = time of the ecological transition/regime shift. Often passed from another function. 
	
	# RETURN: 
	# sampTS = a time series. A 2 col matrix with [,1]=ages and [,2]=ecological value. Produced by subsampling TS2
	
	if (sample.by=="depth"){
		sTS<-TS2[seq(1,nrow(TS2),samp.freq1),]
	} else if (sample.by=="distribute.samples") {
		TStemp<-TS2
		sTS<-TStemp[round(seq(1,nrow(TStemp),length.out=nsamp)),]
	} else {
		print("sample.by must be 'depth' or 'distribute.samples'")
	}
	return(sampTS=sTS)
}

# sampleTSatAC__________________________________________________________
# this function takes samples from a time series at regular intervals, but adds extra samples immediately prior to a specified time point

sampleTSatAC<-function(TS2,AC.buffer,AC.samp,timeCT,start.at){		
	
	# INPUTS:
	# TS2 = a matrix with 2 cols, [,1] is ages, [,2] is ecological value, e.g. tree cover
	# sample.by = should sampling occur at predefined depth increments ("depth") across entire time sereis, or should a set number of samples be distrbuted at regular increments ("distribute.samples") prior to timeCT?
	# AC.buffer: between 0 and 1; proportion of the time series to sample intensively. E.g., if AC.buffer=0.25, function with intensively sample the 25% of the TS immediately prior to the ecological transition
	# AC.samp = proportion of samples to devote to the AC.buffer zone (e.g., AC.samp=0.25 would focuse 25% of smaples on the AC.buffer zone)
	# nsamp = an integer, number of samples of TS2 to take is sample.by="distribute.samples"
	# timeCT = time of the ecological transition/regime shift. Often passed from another function. 
	
	# RETURN: 
	# acTS = a time series. A 2 col matrix with [,1]=ages and [,2]=ecological value. Produced by subsampling TS2
	
	ACbuff<-(max(TS2[,1])-min(TS2[,1]))*AC.buffer	
	rng<-c(((timeCT-ACbuff):timeCT-start.at))
	ACzone<-TS2[intersect(which(TS2[,1] > min(rng)), which(TS2[,1]<max(rng))),]
	
	pass1<-TS2[c(1:(which(TS2[,1]==min(ACzone[,1])))),]
	pass3<-TS2[c((which(TS2[,1]==max(ACzone[,1]))):nrow(TS2)),]
		
	nsamp1<-(nsamp-nsamp*AC.samp)
	nsamp2<-nsamp-nsamp1
	
	seg1<-pass1[seq(1,nrow(pass1),length.out=(nrow(pass1)/nrow(TS2))*nsamp1),]
	seg2<-ACzone[seq(1,nrow(ACzone),length.out=nsamp2),]
	seg3<-pass3[seq(1,nrow(pass3),length.out=(nrow(pass3)/nrow(TS2))*nsamp1),]
			
	AC.sampled<-rbind(seg1,seg2,seg3)
	return(acTS=AC.sampled)
}

# std.sd___________________________________________________________________________
# calculates standard deviation in a rolling window, standardized by the length of time represented by each window

std.sd<-function(tstemp,winlen,step.size){
	
	# INPUTS:
	# tstemp = a time series, as a 2 col matrix where [,1] is time and [,2] is an ecological variable
	# winlen = length of rolling window
	# step.size = increment by which to move the rolling window
	
	# RETURN: 
	# midpoints = vector of rolling window midpoints
	# std.SD = vector of standard deviation values for each rolling window
	# windows2 = a 2 colum matrix; start and end points for each window
	
	samp.points<-seq(1,length(tstemp[,2]),step.size)
	temp1<-cbind(samp.points,(samp.points+winlen)-1)

	temp2<-temp1[which(temp1[,2]<(nrow(tstemp))+1),]
	std.SD<-c()
	midpoints<-c()
	windows2<-matrix(NA,nrow=nrow(temp2),ncol=2)
	for (j in 1:nrow(temp2)){
		rowj<-temp2[j,]
		windows2[j,]<-c(tstemp[rowj[1],1],tstemp[rowj[2],1])
		midpoints.ind<-((rowj[[2]]-rowj[[1]])/2)+rowj[[1]]
		midpoints[j]<-tstemp[midpoints.ind,1]
		wait.time<-tstemp[rowj[[2]],1]-tstemp[rowj[[1]],1]
		win.pol.vals<-tstemp[c(rowj[[1]]:rowj[[2]]),2]				
		std.SD[j]<-sd(win.pol.vals*sqrt(abs(wait.time)))	
	}
	out<-list(midpoints=midpoints, std.SD=std.SD,windows2=windows2)
	return(out)
}


# ACtime___________________________________________________________________	
# calculates autocorrelation time with missing values in a rolling window.

ACtime<-function(tstemp,winlen,step.size){
	
	# INPUTS:
	# tstemp = a time series, as a 2 col matrix where [,1] is time and [,2] is an ecological variable
	# winlen = length of rolling window
	# step.size = increment by which to move the rolling window
	
	# RETURN: 
	# midpoints = vector of rolling window midpoints
	# ACtime = vector of autocrrelation time values for each rolling window
	# windows2 = a 2 colum matrix; start and end points for each window
	
	samp.points<-seq(1,length(tstemp[,2]),step.size)
	temp1<-cbind(samp.points,(samp.points+winlen)-1)
	temp2<-temp1[which(temp1[,2]<(nrow(tstemp))+1),]
	
	#fill gaps with NAs
	pol_vals<-tstemp[,2]
	steps<-1:(max(floor(tstemp[,1])))
	xx<-match(steps,floor(tstemp[,1]),nomatch=9999)
	xxx<-c()
	for (k in 1:length(xx)) {
		if (xx[k]==9999) { xxx[k]<-NA
		} else { xxx[k]<-pol_vals[xx[k]] }
	}

	ACt<-c()
	midpoints<-c()
	windows2<-matrix(NA,nrow=nrow(temp2),ncol=2)
	for (j in 1:nrow(temp2)){
		rowj<-temp2[j,]
		windows2[j,]<-c(tstemp[rowj[1],1],tstemp[rowj[2],1])
		midpoints.ind<-((rowj[[2]]-rowj[[1]])/2)+rowj[[1]]
		midpoints[j]<-tstemp[midpoints.ind,1]
		wait.time<-tstemp[rowj[[2]],1]-tstemp[rowj[[1]],1]
		ac.test<-acf(tstemp[c(rowj[[2]]:rowj[[1]]),2],lag=1,plot=FALSE,na.action=na.pass)
		ACt[j] = -1/log(abs(ac.test$acf[2]))
	}

	out<-list(midpoints=midpoints, ACtime=ACt,windows2=windows2)
	return(out)
}

# ews.summary______________________________________________________________________
# summarizes standard deviation and autocorrelation time rolling window values, and kendall's tau for each

ews.summary<-function(tstemp,winlen,step.size,timeCT){
	
	# INPUTS:
	# tstemp = 2 col matrix; [,1] is age, [,2] is ecological variable 
	# winlen = length of rolling window
	# step.size = increment by which to move the rolling window
	# timeCT = age (in time steps, not as an index) of the regime shift. usually passed from another function
 
	# RETURN:
	# sd.kendall = Kendall's tau values for correlation between standard deviation and time
	# sd.vals = rolling window standard deviation values
	# sd.k.p = p value for standard deviation kendall's tau
	# ac.kendall = Kendall's tau values for correlation between autocorrelation time and time
	# ac.vals = rolling window autocorrelation time values
	# ac.k.p = p value for autocorrelation time kendall's tau

	#calculate SD and Kendall's tau
	tstemp.sd<-std.sd(tstemp,winlen,step.size)
	temp3<-cbind(tstemp.sd$windows2[,2],tstemp.sd$std.SD)
	temp4<-temp3[which(temp3[,1]<timeCT),]
	sd.ken.test<-cor.test(temp4[,1],temp4[,2],method="kendall")
	sd.ken.p<-sd.ken.test$p.value
	sd.ken<-sd.ken.test$estimate[[1]]
	sd.vals<-temp3
	
	#calculateACtime and Kendall's tau
	actemp<-ACtime(tstemp,winlen,step.size)
	temp3<-cbind(actemp$windows2[,2],actemp$ACtime)
	temp4<-temp3[which(temp3[,1]<timeCT),]
	ac.ken.test<-cor.test(temp4[,1],temp4[,2],method="kendall")
	ac.ken.p<-ac.ken.test$p.value
	ac.ken<-ac.ken.test$estimate[[1]]
	ac.vals<-temp3
	
	out<-list(sd.kendall=sd.ken,sd.vals=sd.vals,sd.k.p=sd.ken.p,ac.kendall=ac.ken,ac.vals=ac.vals,ac.k.p=ac.ken.p)
	return(out)
}

# detrendTS__________________________________________________________________________
# detrend time series using linear, gaussian, lowess, or no detrending. Initial function form inspried by the earlywarnings::generic_ews function

detrendTS<-function(oTS,method,bandwidth=NULL,span=NULL, degree=NULL){
	# INPUTS:
	# oTS = generally, a 2 col matrix, with [,1] as time and [,2] as ecological variable. An untransformed single run of the GW model
	# method = should detrending be "linear" (using lm), "gaussian" (using ksmooth function), "lowess" (using loess function), or "none"?
	# bandwidth = bandwidth to use for gaussian detrending. Default is NULL, in which case bandwidth is optimized using bw.nrd0 (following the earlywarnings package)
	# span = span to use for loess smoothing. default is 25/100 (following the earlywarnings package)
	# degree = degree to use for loess smoothign. default is 2 (following the earlywarnings package)
	
	# RETURN:
	# a 2 col matrix with [,1] as time, and [,2] as detrended ecological variable
	
	TS<-oTS[,2]
	timeindex<-oTS[,1]
	if (method=="lowess") {
		if (is.null(span)) {  
			span <- 25/100 
		} else { 
			span <- span/100  
		}
        if (is.null(degree)) {  
        	degree <- 2 
        } else { 
        	degree <- degree   
        }
        smYY <- loess(TS ~ timeindex, span = span, degree = degree, normalize = FALSE, family = "gaussian")
        smY <- predict(smYY, data.frame(x = timeindex), se = FALSE)
        smoothTS <- TS - smY
        out<-cbind(timeindex,smoothTS)
	} else if (method=="gaussian"){
		if (is.null(bandwidth)) { 
			bw <- round(bw.nrd0(timeindex)) 
		} else { 
			bw <- round(length(TS) * bandwidth/100) 
		}
		smYY <- ksmooth(timeindex, TS, kernel = "normal", bandwidth = bw, range.x = range(timeindex), x.points = timeindex)
		smoothTS <- TS - smYY$y
		smY <- smYY$y
		 out<-cbind(timeindex,smoothTS)
    } else if (method == "linear") {
        smoothTS <- resid(lm(TS ~ timeindex))
        smY <- fitted(lm(TS ~ timeindex))
        out<-cbind(timeindex,smoothTS)
	} else if (method=="none") {
		smY<-TS
		smoothTS<-TS
		out<-cbind(timeindex,smoothTS)
	}
	return(out)
}

# multi.ews.summary________________________________________________________________
# summarize rolling window metrics for simulated data

multi.ews.summary<-function(origTS,TAbottom,TAtop,sample.by,nsamp,samp.freq1,samp.freq2,AC.buff,windows,steps,timeCT,TStype,agemodel,breakpoint,detrend_method,a0,a1,cutoff,cutoff2,trim.type,start,sd.pct,AC.samp){
	
	
	# INPUTS:
	# origTS = generally, a 2 col matrix, with [,1] as time and [,2] as ecological variable. Specifically, an untransformed single run of the GW model
	# TAbottom = time averaging in number of years per cm at the bottom (older) of the core (passed to time averaging functions)
	# TAtop = time averaging in number of years per cm at the top (younger) of the core (passed to time averaging functions)
	# sample.by = should sampling occur at predefined depth increments ("depth") across entire time sereis, or should a set number of samples be distrbuted at regular increments ("distribute.samples") prior to timeCT? (passed to time averaging functions)
	# nsamp = an integer, number of samples of TS2 to take is sample.by="distribute.samples"
	# samp.freq1 = an integer, increment at which to sample TS2 if sample.by="depth" (passed to time averaging functions, called "sample.freq1")
	# samp.freq2: an integer, increment at which to sample prior to ecological transition (passed to time averaging functions, called "sample.freq2")
	# AC.buff: between 0 and 1; proportion of the time series to sample intensively. E.g., if AC.buff=0.25, function with intensively sample the 25% of the TS immediately prior to the ecological transition (passed to time averaging functions, where input is called AC.buffer)
	# windows = a vector of rolling window length, corresponding in order to the window size for original, time-averaged, time-averaged and subsampled, time-averaged and targetted subsampled (passed to ews.summary, "winlen")
	# steps = a vector of increments by which to move the rolling window corresponding in order to the window size for original, time-averaged, time-averaged and subsampled, time-averaged and targetted subsampled (passed to ews.summary, "step.size")
	# timeCT = time of the ecological transition/regime shift. Often passed from another function.
	# TStype = corresponding to "TSct" (gradually-forced critical transition), "TSrs" (abruptly-forced critical transition), "TSdc" (gradually forced non-critical transition), or "TSnc" (no change)
	# agemodel = "brokenstick" (calls brokenstick.timeavgTS), "linearTA" (calls timeavgTS), or "Carpenter" (calls Carpenter_timeavgTS)
	# breakpoint = time step (as a irow number index) of transition from TAbottom to TAtop (passed to brokenstick.timeavgTS)
	# detrend_method = detrending method: must be "linear" (using lm), "gaussian" (using ksmooth function), "lowess" (using loess function), or "none" (passed to detrendTS)
	# a0 = compression intercepts parameter (passed to Carpenter_timeavgTS)
	# a1 = compression slope parameter (passed to Carpenter_timeavgTS)
	# cutoff = amount of time to retain prior to regime shift (passed to timetoRS2)
	# cutoff2 = amount of time to retain after regime shift (passed to timetoRS2)
	# trim.type = should the time of regime shift be determined analytically ("to.RS"), arbitarily ("to.set.bounds"), or should no trimming take place ("none")? (passed to timetoRS2)
	# start = if trim.type = "to.set.bounds", start is the first time step to include (passed to timetoRS2)	
	# sd.pct = determines standard deviation of noise around time averaging, as a % of the mean (passed to brokenstick.timeavgTS or timeavgTS)
	# AC.samp = proportion of samples to devote to the AC.buffer zone (e.g., AC.samp=0.25 would focuse 25% of smaples on the AC.buffer zone) (passed to sampleTSatAC)
	
	# RETURN: 
	# sdKs, acKs = vector of Kendall's tau values for standard deviation (sdKs) and autocorrelation time (acKs) in the following order: orignal time series, time-averaged, time-averaged and sampled, time-averaged and targeted sampled
	# sd.vals.multi, ac.vals.multi  = rolling window standard deviaition (sd.vals.multi) and autocorrelation time (ac.vals.multi) values
	# sdKp, acKp = = vector of Kendall's tau p values for standard deviation (sdKs) and autocorrelation time (acKs) in the following order: orignal time series, time-averaged, time-averaged and sampled, time-averaged and targeted sampled
	
	XX<-trimtoRS2(origTS,cutoff,cutoff2,trim.type,start)
	oTS<-XX$trimTS
	
	if (agemodel=="brokenstick"){
		tats1<-brokenstick.timeavgTS(XX$trimTS,TAbottom,TAtop,breakpoint,sd.pct)
	} else if (agemodel=="linearTA") {
		tats1<-timeavgTS(XX$trimTS,TAbottom,TAtop,sd.pct)
	} else if (agemodel=="Carpenter"){
		tats1<-Carpenter_timeavgTS(XX$trimTS,a0,a1,XX$tail.length)
	} else {
		print("agemodel must be 'brokenstick', 'linearTA', or 'Carpenter'")
	}
	
	ta.time<-tats1$adTS[,1]
	adTS<-cbind(ta.time,tats1$adTS[,2])
	regTS<-sampleTS(adTS,sample.by,samp.freq1=NULL,nsamp,timeCT)
	
	if (TStype=="TSnc" || TStype=="TSdc"){
		acTS<-sampleTSatAC(adTS,AC.buffer=AC.buff,AC.samp=AC.samp,exRStime,start.at=XX$tail.length)
	} else {
		acTS<-sampleTSatAC(adTS,AC.buffer=AC.buff,AC.samp=AC.samp,timeCT,start.at=XX$tail.length)
	}
	
	oTS<-detrendTS(oTS,method=detrend_method)
	adTS<-detrendTS(adTS,method=detrend_method)
	regTS<-detrendTS(regTS,method=detrend_method)
	acTS<-detrendTS(acTS,method=detrend_method)
	
	ews.orig<-ews.summary(oTS,windows[1],steps[1],timeCT)
	ews.ad<-ews.summary(adTS,windows[2],steps[2],timeCT)
	ews.reg<-ews.summary(regTS,windows[3],steps[3],timeCT)
	ews.ac<-ews.summary(acTS,windows[4],steps[4],timeCT)
	
	sdKs<-c(ews.orig$sd.kendall,ews.ad$sd.kendall,ews.reg$sd.kendall,ews.ac$sd.kendall)
	sdKp<-c(ews.orig$sd.k.p,ews.ad$sd.k.p,ews.reg$sd.k.p,ews.ac$sd.k.p)
	sd.vals.multi<-list(ews.orig$sd.vals,ews.ad$sd.vals,ews.reg$sd.vals,ews.ac$sd.vals)
	names(sdKs)<-c("orig","ad","reg","ac")
	names(sd.vals.multi)<-c("orig","ad","reg","ac")
	names(sdKp)<-c("orig","ad","reg","ac")

	acKs<-c(ews.orig$ac.kendall,ews.ad$ac.kendall,ews.reg$ac.kendall,ews.ac$ac.kendall)
	acKp<-c(ews.orig$ac.k.p,ews.ad$ac.k.p,ews.reg$ac.k.p,ews.ac$ac.k.p)
	ac.vals.multi<-list(ews.orig$ac.vals,ews.ad$ac.vals,ews.reg$ac.vals,ews.ac$ac.vals)
	names(acKs)<-c("orig","ad","reg","ac")
	names(ac.vals.multi)<-c("orig","ad","reg","ac")
	names(acKp)<-c("orig","ad","reg","ac")
	
	out<-list(sdKs=sdKs,acKs=acKs,sd.vals.multi=sd.vals.multi,ac.vals.multi=ac.vals.multi,sdKp=sdKp,acKp=acKp)
	return(out)
}


# rep.ews________________________________________________________________________	
# summarize resilience indicators for multiple simulations

rep.ews<-function(TStype,nreps, TAbottom,TAtop,sample.by,nsamp,samp.freq1,samp.freq2,AC.buff,windows,steps,agemodel,breakpoint,det_method,a0,a1,cutoff,cutoff2,trim.type,start,q,sd.pct,AC.samp){
	
	# INPUTS:
	# TStype = corresponding to "TSct" (gradually-forced critical transition), "TSrs" (abruptly-forced critical transition), "TSdc" (gradually forced non-critical transition), or "TSnc" (no change)
	# nreps = number of simulations
	# TAbottom = time averaging in number of years per cm at the bottom (older) of the core
	# TAtop = time averaging in number of years per cm at the top (younger) of the core
	# sample.by = should sampling occur at predefined depth increments ("depth") across entire time sereis, or should a set number of samples be distrbuted at regular increments ("distribute.samples") prior to timeCT? (passed to multi.ews.summary)
	# nsamp = an integer, number of samples of TS2 to take is sample.by="distribute.samples"
	# samp.freq1 = an integer, increment at which to sample TS2 if sample.by="depth" (passed multi.ews.summary)
	# samp.freq2: an integer, increment at which to sample prior to ecological transition (passed multi.ews.summary)
	# AC.buff: between 0 and 1; proportion of the time series to sample intensively. E.g., if AC.buff=0.25, function with intensively sample the 25% of the TS immediately prior to the ecological transition (passed to multi.ews.summary)
	# windows = a vector of rolling window length, corresponding in order to the window size for original, time-averaged, time-averaged and subsampled, time-averaged and targetted subsampled (passed to multi.ews.summary)
	# steps = a vector of increments by which to move the rolling window corresponding in order to the window size for original, time-averaged, time-averaged and subsampled, time-averaged and targetted subsampled (passed to multi.ews.summary)
	# agemodel = "brokenstick" (calls brokenstick.timeavgTS), "linearTA" (calls timeavgTS), or "Carpenter" (passed to multi.ews.summary)
	# breakpoint = time step (as a irow number index) of transition from TAbottom to TAtop (passed to multi.ews.summary)
	# det_method = detrending method: must be "linear" (using lm), "gaussian" (using ksmooth function), "lowess" (using loess function), or "none" (passed to multi.ews.summary, "detrend_method")
	# a0 = compression intercepts parameter (passed to multi.ews.summary)
	# a1 = compression slope parameter (passed to multi.ews.summary)
	# cutoff = amount of time to retain prior to regime shift (passed to multi.ews.summary)
	# cutoff2 = amount of time to retain after regime shift (passed to multi.ews.summary)
	# trim.type = should the time of regime shift be determined analytically ("to.RS"), arbitarily ("to.set.bounds"), or should no trimming take place ("none")?  (passed to multi.ews.summary)
	# start = if trim.type = "to.set.bounds", start is the first time step to include (passed to multi.ews.summary)
	# q = hill slope parameter for fire mortality (passed to single_run)
	# sd.pct = determines standard deviation of noise around time averaging, as a % of the mean (passed to multi.ews.summary)
	# AC.samp = proportion of samples to devote to the AC.buffer zone (e.g., AC.samp=0.25 would focuse 25% of smaples on the AC.buffer zone)(passed to multi.ews.summary)
	
	#RETURNS:
	# sd.kendalls = a 4 x nreps column matrix, standard deviation kendall's tau values 
	# ac.kendalls = a 4 x nreps column matrix, autocorrelation time kendall's tau values
	# rep.sd.vals = a list, standard deviation rolling window values
	# rep.ac.vals = a list, autocorrelation time rolling window values
	# timeCTs = time of regime shift for each simulation
	# rep.sd.ps = a 4 x nreps column matrix, standard deviation kendall's p values
	# rep.ac.ps = a 4 x nreps column matrix, autocorrelation time kendall's p values
			
	rep.sdKs<-matrix(NA,nrow=nreps,ncol=4)
	rep.sd.vals<-list()
	rep.sd.ps<-matrix(NA,nrow=nreps,ncol=4)
	
	rep.acKs<-matrix(NA,nrow=nreps,ncol=4)
	rep.ac.vals<-list()
	rep.ac.ps<-matrix(NA,nrow=nreps,ncol=4)

	timeCTs<-c()
	
	if ((TStype %in% c("TSct","TSrs","TSdc","TSnc"))==FALSE) {
		print("TStype must be 'TSct', 'TSrs', 'TSnc', or 'TSdc'")
	}
	
	if (TStype=="TSrs"){
		driver_topo<-"abrupt"
		pulse_time=exRStime
	} else {
		driver_topo<-"gradual"
		pulse_time=pulse_time
	}
		
	for (i in 1:nreps){
			print(i)
			
			single_test = single_run(r=r, gens=gens, delta_t=delta_t, K_Start=K_Start, K_Pulse_amt=K_Pulse_amt, V0=V0, pulse_time=pulse_time,driver_press_topo=driver_topo,q=q)
			
			TS<-single_test[,3]
			origTS<-cbind(c(1:length(TS)),TS)
			
		if (TStype=="TSct" || TStype=="TSrs"){
			bp.out<-CE.Normal.Mean(as.data.frame(TS),Nmax=1)
			timeCT<-bp.out$BP.Loc
		} else {
			timeCT<-exRStime
		}
			
		timeCTs[i]<-timeCT
			
		summary.temp<-multi.ews.summary(origTS,TAbottom,TAtop,sample.by,nsamp,samp.freq1,samp.freq2,AC.buff,windows,steps,timeCT,TStype=TStype,agemodel,breakpoint,det_method,a0,a1,cutoff,cutoff2,trim.type,start,sd.pct,AC.samp)
			
		rep.sdKs[i,]<-summary.temp$sdKs
		rep.sd.vals[[i]]<-summary.temp$sd.vals.multi
		rep.sd.ps[i,]<-summary.temp$sdKp
			
		rep.acKs[i,]<-summary.temp$acKs
		rep.ac.vals[[i]]<-summary.temp$ac.vals.multi
		rep.ac.ps[i,]<-summary.temp$acKp
	}
	
	out<-list(sd.kendalls=rep.sdKs,ac.kendalls=rep.acKs,rep.sd.vals=rep.sd.vals,rep.ac.vals=rep.ac.vals,timeCTs=timeCTs,rep.sd.ps=rep.sd.ps,rep.ac.ps=rep.ac.ps)
	
	return(out)
}

# calcROC_______________________________________________________________
# function for calculating ROC for two distributions of Kendall's tau values, modified from analogue::roc 

calcROC <- function(IN, OUT, max.len = 10000) {
		# INPUTS:
		# IN = values for the the first distribution
		# OUT = values for the the second distribution
		# max.len = numeric; length of analolgue and non-analogue vectors. Used as limit to thin points on ROC curve to (from analogue::roc)
		
		# RETURNS:
		# TPF = the true positive fraction
		# FPE = the false positive error
		# optimal  = optimal dissimilarity value where slope of the ROC curve is maximal
		# AUC = area under the ROC curve
		# se.fit = standard error of the AUC estimate
		# n.in = numeric, number of samples within the current group
		# n.out = numeric, number of samples not in the current group
		# p.value = p value of a wilcoxon rank sum test on the two sets of dissimilarities. Also known as a Mann-Whitney test
		# roc.points =  the unique dissimilarities at which the roc curve was evaluated
		# max.roc = position along the ROC curve at which the slope of the ROC curve is maximal. this is the index of this point on the curve
		# prior = numeric, length 2. Vector of observed probabilities of true analogue and true non-analogues in the group
		# analogue = a list with componenents yes and no containing the dissimilarities for true analgoe and true non-analogues in the group
		
    	n.IN <- length(IN)
    	n.OUT <- length(OUT)
    	g <- rep(c(TRUE, FALSE), times = c(n.IN, n.OUT))
    	tab <- table(c(IN, OUT), g)
    	TPF <- cumsum(tab[, 2])/sum(tab[, 2])
    	FPE <- cumsum(tab[, 1])/sum(tab[, 1])
    	roc.values <- TPF - FPE
    	roc.points <- rev(sort(as.numeric(dimnames(tab)[[1]])))
    	optimal <- as.numeric(names(max.roc <- which.max(abs(roc.values))))
    	names(FPE) <- names(TPF) <- names(roc.values) <- NULL
    	wilcox <- wilcox.test(IN, OUT, conf.int = FALSE)
    	AUC <- 1 - (wilcox$statistic/(n.IN * n.OUT))
    #AUC <- (wilcox$statistic/(n.IN * n.OUT))
     AUC2 <- AUC^2
    	q1 <- AUC/(2 - AUC)
    	q2 <- (2 * AUC2)/(1 + AUC)
    	se.fit <- AUC * (1 - AUC) + ((n.IN - 1) * (q1 - AUC2)) + ((n.OUT - 1) * (q2 - AUC2))
    	se.fit <- sqrt(se.fit/(n.IN * n.OUT))
    	p.value <- wilcox$p.value
    	prior <- c(n.IN, n.OUT)/sum(n.IN, n.OUT)
    	retval <- list(TPF = TPF, FPE = FPE, optimal = optimal, AUC = AUC, se.fit = se.fit, n.in = n.IN, n.out = n.OUT, p.value = p.value, roc.points = roc.points, max.roc = max.roc, prior = prior, analogue = list(yes = IN, no = OUT))
    	retval
    }

# plot.taph.hists_______________________________________________________________________
# plot histograms of Kendall's tau values with ROC and optimum discriminant threshold

plot.taph.hists<-function(Xct,Xdc,Xrs,Xnc,indicator,yaxis,mains,ymax,labs2,letters,title,type.label,taph.ind){
	# INPUTS:
	# Xct, Xdc, Xrs, Xnc = simulation outputs from rep.ews function. Not necessarily, but generally: Xct = return for gradually-forced critical transitions, Xdc = = return for gradually-forced non-critical transitions, Xrs = return for abruptly-forced critical transitions, Xnc = return for no change
	# indicator = "sd" to choose standard deviation, or "ac" to choose autocorrelation time
	# yaxis = T/F, should the y axis be plotted? 
	# mains = list of headings
	# ymax = maximum y axis value
	# labs2 = labels for each tile
	# letters = letters for each tile
	# title = title for entire set of plots
	# type.label = T/F, should the time series type name be plotted on the right-hand axis?
	# taph.ind = interger between 1 and 4. 1 = untransformed, 2 = age-depth transformed, 3 = age-depth and sampling transformed, 4 = age-depth and targeted sampling transformed
	
	bins<-seq(-1,1,0.1)
	prob.table<-list()
	for (i in taph.ind){
		ts.list<-c("orig","ad","reg","ac")
		if (indicator=="sd"){
			kendalls<-"sd.kendalls"
		} else if (indicator=="ac"){
			kendalls<-"ac.kendalls"
		} else { print("indicator must be 'sd' or 'ac'")	}

		nreps<-length(Xnc[[kendalls]][,i])
		
		A<-hist(Xct[[kendalls]][,i],breaks=bins,plot=F)
		Aprops<-A$counts/nreps
		B<-hist(Xdc[[kendalls]][,i],plot=FALSE,breaks=bins)
		Bprops<-B$counts/nreps
		C<-hist(Xrs[[kendalls]][,i],plot=FALSE,breaks=bins)
		Cprops<-C$counts/nreps
		D<-hist(Xnc[[kendalls]][,i],plot=FALSE,breaks=bins)
		Dprops<-D$counts/nreps
		counts<-cbind(A$counts,B$counts,C$counts,D$counts)
		
		type.labels<-c("Grad. CT","Grad. nonCT","Abrupt CT","No Change")
		prop.list<-list(Aprops,Bprops,Cprops,Dprops)
		colors<-c(colorCT,colorDC,colorRS,colorNC)
		OUTS<-list(c(Xrs[[kendalls]][,i],Xdc[[kendalls]][,i],Xnc[[kendalls]][,i]),Xdc[[kendalls]][,i],Xrs[[kendalls]][,i],Xnc[[kendalls]][,i])
		
		for (j in 1:4){
			
			barplot(prop.list[[j]],ylim=c(0,ymax),main="",col=colors[j],cex.axis=1.2,cex.main=1.5,las=1,yaxt="n")
						
			IN<-Xct[[kendalls]][,i]
			OUT<-OUTS[[j]]
			
			if (j==1) {
				mtext(title,3,line=0.5,cex=1)
				LTY=1
			} else { LTY=3 }
			
			
			if (min(IN)>max(OUT)) {
				text(-1,ymax*0.6,"No overlap",pos=4,cex=1)
			} else {
				roc<-calcROC(IN, OUT, max.len = 10000)
				if (roc$AUC<0.5){ 
					AUC<-1-round(roc$AUC,3)
				} else { AUC<-round(roc$AUC,3) }
				
				opt.interp<-approx(c(-1,1),c(0,24),xout=roc$optimal)
				auc<-paste(AUC,"+-",round(roc$se.fit,2))
				abline(v=opt.interp$y,lty=LTY,lwd=1.5,col="gray30")
				
				if (round(roc$optimal,3)<0){ xloc=-1
				} else { xloc=-1 }
				
				text(xloc,ymax*0.6,paste("AUC =", auc),pos=4,cex=1.1)
				text(xloc,ymax*0.45,paste("optimum =", round(roc$optimal,3)),pos=4,cex=1.1)
			}
			
			if (yaxis==T) { axis(2,las=1,hadj=0.75,tcl=-0.25,cex.axis=1.2) 
			} else { axis(2,las=1,tcl=-0.25,labels=F,cex.axis=1.2) }
			if (type.label==T){ mtext(type.labels[j],4,outer=F,cex=1) }
				
			text(-0.9,ymax*0.9,letters[j],pos=4,cex=1.5)
			#mtext(mains[1],3,line=-2,cex=0.7)
		}

		temp.table<-round(t(apply(counts,1,function(x) x/sum(x))),digits=4)	
		mids<-barplot(t(temp.table),plot=FALSE)
		mstep<-(mids[2]-mids[1])/2
		locs<-mids-(mids[2]-mids[1])/2
		step<-locs[2]-locs[1]
		at.vect<-c(locs,locs[length(locs)]+step)
		locs3<-at.vect[seq(1,length(at.vect),2)]
		axis(1,at=locs3,labels=seq(-1,1,0.2),tick=T,las=2,cex.axis=1.2,line=0.25,mgp=c(3,0.5,0.5),tcl=-0.25)	
	}
}

# multi.ks.test_________________________________________
multi.ks.test<-function(Xct,Xdc,Xrs,Xnc,ts,indicator, p.correct,alpha){
	# INPUTS:
	# Xct, Xdc, Xrs, Xnc = simulation outputs from rep.ews function. Not necessarily, but generally: Xct = return for gradually-forced critical transitions, Xdc = = return for gradually-forced non-critical transitions, Xrs = return for abruptly-forced critical transitions, Xnc = return for no change
		
		
		if (ts=="orig"){ ts.ind<-1 } 
		else if (ts=="ad"){ ts.ind<-2 } 
		else if (ts=="reg"){ ts.ind<-3 }
		else if (ts=="ac"){ ts.ind<-4 }
		else { print("ts must be 'orig','ad','reg',or'ac'") }
		
		if (indicator=="sd"){
			kens<-"sd.kendalls"
			ken.ps<-"rep.sd.ps"
		} else if (indicator=="sk"){
			kens<-"sk.kendalls"
			ken.ps<-"rep.sk.ps"
		} else if (indicator=="ar"){
			kens<-"ar.kendalls"
			ken.ps<-"rep.ar.ps"
		} else if (indicator=="ac"){
			kens<-"ac.kendalls"
			ken.ps<-"rep.ac.ps"
		} else { print("indicator must be 'sd', 'sk', 'ar', or 'ac'") }
		
		CTvDC<-ks.test(Xct[[kens]][,ts.ind],Xdc[[kens]][,ts.ind])
		CTvRS<-ks.test(Xct[[kens]][,ts.ind],Xrs[[kens]][,ts.ind])
		CTvNC<-ks.test(Xct[[kens]][,ts.ind],Xnc[[kens]][,ts.ind])
		DCvRS<-ks.test(Xdc[[kens]][,ts.ind],Xrs[[kens]][,ts.ind])
		DCvNC<-ks.test(Xdc[[kens]][,ts.ind],Xnc[[kens]][,ts.ind])
		RSvNC<-ks.test(Xrs[[kens]][,ts.ind],Xnc[[kens]][,ts.ind])

		ps<-c(CTvDC$p.value,CTvRS$p.value,CTvNC$p.value,DCvRS$p.value,DCvNC$p.value,RSvNC$p.value)
				
		if (p.correct==TRUE){
			ps.out<-p.adjust(ps, method="bonferroni")
			ps.out<-t(ps.out)
		} else { ps.out<-ps }
		
		ind.stats<-matrix(NA,nrow=4,ncol=9)
		ind.stats[1,]<-c(summary(Xct[[kens]][,ts.ind]),quantile(Xct[[kens]][,ts.ind],0.025),quantile(Xct[[kens]][,ts.ind],0.975),sum(Xct[[ken.ps]][,ts.ind]<alpha)/length(Xct[[ken.ps]][,ts.ind])*100)
		ind.stats[2,]<-c(summary(Xdc[[kens]][,ts.ind]),quantile(Xdc[[kens]][,ts.ind],0.025),quantile(Xdc[[kens]][,ts.ind],0.975),sum(Xdc[[ken.ps]][,ts.ind]<alpha)/length(Xdc[[ken.ps]][,ts.ind])*100)
		ind.stats[3,]<-c(summary(Xrs[[kens]][,ts.ind]),quantile(Xrs[[kens]][,ts.ind],0.025),quantile(Xrs[[kens]][,ts.ind],0.975),sum(Xrs[[ken.ps]][,ts.ind]<alpha)/length(Xrs[[ken.ps]][,ts.ind])*100)
		ind.stats[4,]<-c(summary(Xnc[[kens]][,ts.ind]),quantile(Xnc[[kens]][,ts.ind],0.025),quantile(Xnc[[kens]][,ts.ind],0.975),sum(Xnc[[ken.ps]][,ts.ind]<alpha)/length(Xnc[[ken.ps]][,ts.ind])*100)
		colnames(ind.stats)<-c("Min.","1st Qu","Median","Mean","3rd Qu.","Max.","2.5% CI","97.5% CI","% Significant")
		rownames(ind.stats)<-c("TSct","TSdc","TSrs","TSnc")
		
		indp<-matrix(NA,nrow=3,ncol=3)
		indp[,1]<-ps.out[c(1:3)]
		indp[,2]<-c(NA,ps.out[c(4,5)])
		indp[,3]<-c(NA,NA,ps.out[6])
		colnames(indp)<-c("CT","DC","RS")
		rownames(indp)<-c("DC","RS","NC")
		
		out<-list(ps.out=ps.out,ind.stats=ind.stats,p.table=indp)
		return(out)
	}
	
# Kt.summary.stats_________________________________________
# generate summary stats for simulation Kendall's tau values

Kt.summary.stats<-function(Xct,Xdc,Xrs,Xnc,digits,indicator){
	# INPUTS:
	# Xct, Xdc, Xrs, Xnc = simulation outputs from rep.ews function. Not necessarily, but generally: Xct = return for gradually-forced critical transitions, Xdc = = return for gradually-forced non-critical transitions, Xrs = return for abruptly-forced critical transitions, Xnc = return for no change
	# digits = digits to roud to
	# indicator = "sd" for standard deviaition or "ac" for autocorrelation time
	
	# RETURN:
	# a matrix; summary statistic for each simulation type
	
	t1<-multi.ks.test(Xct,Xdc,Xrs,Xnc,ts="orig",indicator,p.correct=F,alpha=0.05)
	t2<-multi.ks.test(Xct,Xdc,Xrs,Xnc,ts="ad",indicator,p.correct=F,alpha=0.05)
	t3<-multi.ks.test(Xct,Xdc,Xrs,Xnc,ts="reg",indicator,p.correct=F,alpha=0.05)
	t4<-multi.ks.test(Xct,Xdc,Xrs,Xnc,ts="ac",indicator,p.correct=F,alpha=0.05)
	col1<-c(rep("Untransformed",4),rep("AD",4),rep("AD+SAMP",4),rep("AD+TSAMP",4))
	col2<-rep(c("CT","LD","RS","NC"),4)
	
	temp.stats<-rbind(t1$ind.stats,t2$ind.stats,t3$ind.stats,t4$ind.stats)
	temp.stats<-round(temp.stats,digits)
	out<-cbind(col1,col2,temp.stats)
	return(out)
}

# plot.supp_________________________________________
# plot supplemental figures in a grid

plot.supp<-function(Xct,Xdc,Xrs,Xnc,ind,main){
	# INPUTS:
	# Xct, Xdc, Xrs, Xnc = simulation outputs from rep.ews function. Not necessarily, but generally: Xct = return for gradually-forced critical transitions, Xdc = = return for gradually-forced non-critical transitions, Xrs = return for abruptly-forced critical transitions, Xnc = return for no change
	# ind = "sd" for standard deviaition or "ac" for autocorrelation time
	# main = a tilte for the plot
	
	colorCT<-rgb(0.1,0.3,0.4,1)
	colorDC<-rgb(0.1,0.3,0.4,0.7)
	colorRS<-rgb(0.1,0.3,0.4,0.5)
	colorNC<-rgb(0.1,0.3,0.4,0.3)
	dev.new(width=8,height=4.5)
	par(oma=c(6,4,4,1),mar=c(0.75,0.5,0,0.5))
	nf<-layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16),nrow=4,ncol=4,byrow=F))

plot.taph.hists(Xct,Xdc,Xrs,Xnc,indicator=ind,yaxis=T,mains=mains2,ymax=1.05,labs2=NULL,letters=c("a)","b)","c)","d)"),title="Untransformed",type.label=F,taph.ind=1)
plot.taph.hists(Xct,Xdc,Xrs,Xnc,indicator=ind,yaxis=F,mains=mains2,ymax=1.05,labs2=NULL,letters=c("e)","f)","g)","h)"),title="Age-Depth",type.label=F,taph.ind=2)
plot.taph.hists(Xct,Xdc,Xrs,Xnc,indicator=ind,yaxis=F,mains=mains2,ymax=1.05,labs2=NULL,letters=c("i)","j)","k)","l)"),title="AD+Even Samp",type.label=F,taph.ind=3)
plot.taph.hists(Xct,Xdc,Xrs,Xnc,indicator=ind,yaxis=F,mains=mains2,ymax=1.05,labs2=NULL,letters=c("m)","n)","o)","p)"),title="AD+Targeted Samp",type.label=F,taph.ind=4)

	mtext(main,3,line=1.75,outer=T)
	mtext("Frequency",2,line=2,outer=T,cex=1.1)
	mtext("Kendall's tau",1,line=2.25,outer=T,cex=1.1)
}

# END FUNCTIONS ###########################################################

